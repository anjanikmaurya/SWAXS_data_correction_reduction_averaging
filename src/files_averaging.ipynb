{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe348e0",
   "metadata": {},
   "source": [
    "This notebook performs file averaging, using fnmatch patterns in variable averaging_patterns, taking those\n",
    "files, and averaging them together. You will need to specify the fnmatch patterns to perform this file averaging at the bottom.\n",
    "\n",
    "REVISIT: Should I average PDIs or just CSVs?\n",
    "REVISIT: I want to move the dat file and metadata reading function to a different directory. Would helper_code be good?\n",
    "- It can help us with plotting, file averaging, and ensuring accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8dc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f314b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: larger_test/1D\n",
      "Using averaging patterns: 4 pattern groups\n",
      "  Group 1: ['*Hor_scan_Run4*']\n",
      "  Group 2: ['*Run10_AcOH*T20*ctr0*']\n",
      "  Group 3: ['*Run10_AcOH*T22*ctr1*']\n",
      "  Group 4: ['*Run11*PS_AcOH*T157*ctr55*']\n",
      "SAXS Reduction directory not found: larger_test/1D/SAXS/Reduction\n",
      "WAXS Reduction directory not found: larger_test/1D/WAXS/Reduction\n",
      "\n",
      "Averaging complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def group_files_by_fnmatch_patterns(file_list: list[Path], pattern_groups: list) -> dict:\n",
    "    \"\"\"\n",
    "    Groups files based on fnmatch pattern groups (similar to Step1 notebook approach)\n",
    "    Input: file_list: List of file paths\n",
    "           pattern_groups: List of pattern lists (e.g., [[\"*Run6*RampT*\"], [\"*Run7*RampT*\"]]).\n",
    "           Each pattern corresponds to a single group\n",
    "    Output: Dictionary mapping base filenames to list of file paths\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    matched_files = set()  # Track which files have been matched to avoid duplicates\n",
    "\n",
    "    for pattern_group in enumerate(pattern_groups):\n",
    "        group_files = []\n",
    "\n",
    "        # For each pattern in the group, find matching files\n",
    "        for pattern in pattern_group[1]:  # pattern_group is (index, patterns_list)\n",
    "            for file_path in file_list:\n",
    "                if file_path not in matched_files and fnmatch.fnmatch(file_path.name, pattern):\n",
    "                    group_files.append(file_path)\n",
    "                    matched_files.add(file_path)\n",
    "\n",
    "        # Only create a group if files were found\n",
    "        if group_files:\n",
    "            # Use base filename from first file (without extension) as group name\n",
    "            first_file = group_files[0]\n",
    "            group_name = first_file.stem  # Gets filename without extension\n",
    "            groups[group_name] = group_files\n",
    "        else:\n",
    "            print(f\"Warning: group {pattern_group} did not match any patterns:\")\n",
    "\n",
    "    unmatched_files = [f for f in file_list if f not in matched_files]\n",
    "    if unmatched_files:\n",
    "        print(f\"{len(unmatched_files)} files did not match any patterns:\")\n",
    "\n",
    "    return groups\n",
    "\n",
    "def average_dat_files(file_paths: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Averages multiple .dat files using simple column averaging\n",
    "    Input: List of .dat file paths to average\n",
    "    Output: (header_lines, avg_q, avg_intensity, avg_sigma, averaged_metadata)\n",
    "    \"\"\"\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No files to average\")\n",
    "\n",
    "    # Use header from first file and collect metadata\n",
    "    header_lines, first_q, first_intensity, first_sigma, first_metadata = read_dat_data_metadata(file_paths[0])\n",
    "\n",
    "    # Initialize arrays for averaging\n",
    "    all_q = [first_q]\n",
    "    all_intensity = [first_intensity]\n",
    "    all_sigma = [first_sigma]\n",
    "    all_metadata = [first_metadata]\n",
    "\n",
    "    # Read remaining files\n",
    "    for file_path in file_paths[1:]:\n",
    "        _, q_vals, intensity_vals, sigma_vals, metadata = read_dat_data_metadata(file_path)\n",
    "        all_q.append(q_vals)\n",
    "        all_intensity.append(intensity_vals)\n",
    "        all_sigma.append(sigma_vals)\n",
    "        all_metadata.append(metadata)\n",
    "\n",
    "    # Convert to numpy arrays and average data\n",
    "    all_q = np.array(all_q)\n",
    "    all_intensity = np.array(all_intensity)\n",
    "    all_sigma = np.array(all_sigma)\n",
    "\n",
    "    # Simple averaging across files (axis=0)\n",
    "    avg_q = np.mean(all_q, axis=0)\n",
    "    avg_intensity = np.mean(all_intensity, axis=0)\n",
    "    avg_sigma = np.mean(all_sigma, axis=0)\n",
    "\n",
    "    # Average metadata\n",
    "    averaged_metadata = {}\n",
    "    if all_metadata:\n",
    "        # Get all unique keys from all metadata dictionaries\n",
    "        all_keys = set()\n",
    "        for metadata in all_metadata:\n",
    "            all_keys.update(metadata.keys())\n",
    "\n",
    "        # Average each key across all files\n",
    "        for key in all_keys:\n",
    "            values = []\n",
    "            for metadata in all_metadata:\n",
    "                if key in metadata:\n",
    "                    values.append(metadata[key])\n",
    "                else:\n",
    "                    # Throw error if any file is missing metadata after averaging\n",
    "                    raise ValueError(f\"Metadata key '{key}' missing from file {file_paths[all_metadata.index(metadata)]}\")\n",
    "\n",
    "            if values:\n",
    "                averaged_metadata[key] = np.mean(values)\n",
    "\n",
    "    return header_lines, avg_q, avg_intensity, avg_sigma, averaged_metadata\n",
    "\n",
    "\n",
    "def save_averaged_data(header_lines: list, q, intensity, sigma, averaged_metadata: dict, output_path: Path):\n",
    "    \"\"\"\n",
    "    Saves averaged data in .dat format with original header and averaged metadata\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        # Write header comments from first file\n",
    "        for line in header_lines:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "        # Write data\n",
    "        for i in range(len(q)):\n",
    "            f.write(f\"  {q[i]:e}    {intensity[i]:e}   {sigma[i]:e}\\n\")\n",
    "\n",
    "        # Write averaged metadata section\n",
    "        if averaged_metadata:\n",
    "            f.write(\"# METADATA INFORMATION (YML FORMAT, AVERAGED)\\n\")\n",
    "            for key, value in averaged_metadata.items():\n",
    "                f.write(f\"# {key}: {value}\\n\")\n",
    "\n",
    "\n",
    "def average_files_in_directory(input_dir: Path, detector_type: str, pattern_groups: list):\n",
    "    \"\"\"\n",
    "    Takes in an individual SAXS/Reduction or WAXS/Reduction directory and performs averaging\n",
    "    for files and writes averaged files to SAXS/Averaged or WAXS/Averaged\n",
    "    \"\"\"\n",
    "    # Get all .dat files in the directory\n",
    "    dat_files = list(input_dir.glob(\"*.dat\"))\n",
    "    \n",
    "    if not dat_files:\n",
    "        print(f\"No .dat files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    file_groups = group_files_by_fnmatch_patterns(dat_files, pattern_groups)\n",
    "\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = input_dir.parent / \"Averaged\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing {len(file_groups)} groups in {input_dir}\")\n",
    "    \n",
    "    # Process each group\n",
    "    for group_key, files_in_group in file_groups.items():\n",
    "        print(f\"  Averaging {len(files_in_group)} files for pattern: {group_key}\")\n",
    "\n",
    "        # Average the files\n",
    "        header_lines, avg_q, avg_intensity, avg_sigma, averaged_metadata = average_dat_files(files_in_group)\n",
    "\n",
    "        # Create output filename using base filename + \"_Averaged\"\n",
    "        output_filename = f\"{group_key}_Averaged.dat\"\n",
    "        output_path = output_dir / output_filename\n",
    "\n",
    "        # Save averaged data\n",
    "        save_averaged_data(header_lines, avg_q, avg_intensity, avg_sigma, averaged_metadata, output_path)\n",
    "\n",
    "        print(f\"    Saved: {output_path}\")\n",
    "            \n",
    "def process_directory(base_dir: Path):\n",
    "    \"\"\"\n",
    "    Main function to process both SAXS and WAXS directories using fnmatch patterns\n",
    "    Input: Path to 1D/ directory containing SAXS/ and WAXS/ subdirectories\n",
    "    \"\"\"\n",
    "    saxs_reduction_dir = base_dir / \"SAXS\" / \"Reduction\"\n",
    "    waxs_reduction_dir = base_dir / \"WAXS\" / \"Reduction\"\n",
    "    \n",
    "    print(f\"Processing directory: {base_dir}\")\n",
    "    print(f\"Using averaging patterns: {len(averaging_patterns)} pattern groups\")\n",
    "    for i, patterns in enumerate(averaging_patterns):\n",
    "        print(f\"  Group {i+1}: {patterns}\")\n",
    "    \n",
    "    if saxs_reduction_dir.exists():\n",
    "        print(\"\\nProcessing SAXS files...\")\n",
    "        average_files_in_directory(saxs_reduction_dir, \"SAXS\", averaging_patterns)\n",
    "    else:\n",
    "        print(f\"SAXS Reduction directory not found: {saxs_reduction_dir}\")\n",
    "    \n",
    "    if waxs_reduction_dir.exists():\n",
    "        print(\"\\nProcessing WAXS files...\")\n",
    "        average_files_in_directory(waxs_reduction_dir, \"WAXS\", averaging_patterns)\n",
    "    else:\n",
    "        print(f\"WAXS Reduction directory not found: {waxs_reduction_dir}\")\n",
    "    \n",
    "    print(\"\\nAveraging complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the entire 1D directory\n",
    "\n",
    "averaging_patterns = [\n",
    "['*Hor_scan_Run4*'],\n",
    "['*Run10_AcOH*T20*ctr0*'],\n",
    "['*Run10_AcOH*T22*ctr1*'],\n",
    "['*Run11*PS_AcOH*T157*ctr55*']\n",
    "]\n",
    "process_directory(Path(\"larger_test/1D/\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swaxs-data-reduction-correction-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
