{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be27eec",
   "metadata": {},
   "source": [
    "# BL 1-5: SAXS data correction and reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65355923",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Created on Fri Apr 21 23:41:02 2023\n",
    "\n",
    "@author: akmaurya\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc2b8d",
   "metadata": {},
   "source": [
    "Global stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from axis_plot_prop import axis_plot_prop \n",
    "import pyFAI\n",
    "import shutil\n",
    "import fabio\n",
    "from pyFAI.gui.jupyter.calib import Calibration\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89935775",
   "metadata": {},
   "source": [
    "# Common functions for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot setting fucntion\n",
    "\n",
    "def axis_plot_prop(ax):\n",
    "    \"\"\"\n",
    "    Set axis properties for plotting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib axis object\n",
    "        The axis object to set properties for.\n",
    "    \"\"\"\n",
    "\n",
    "    co = [\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0.44, 0.00, 0.99],\n",
    "        [1.00, 0.50, 0.10],\n",
    "        [0.75, 0.00, 0.75],\n",
    "        [0.50, 0.50, 0.50],\n",
    "        [0.50, 0.57, 0.00],\n",
    "        [0.64, 0.08, 0.18],\n",
    "        [0.93, 0.00, 0.00]\n",
    "    ]\n",
    "\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=co)\n",
    "\n",
    "    ax.set_facecolor('white')\n",
    "    ax.spines['top'].set_linewidth(1)\n",
    "    ax.spines['right'].set_linewidth(1)\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.tick_params(axis='both', which='both', direction='in', length=4, width=1)\n",
    "    ax.minorticks_on()\n",
    "    ax.set_xlabel('X Label', fontsize=20, fontname='Arial')\n",
    "    ax.set_ylabel('Y Label', fontsize=20, fontname='Arial')\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.legend(frameon=False)\n",
    "    #ax.set_box_aspect(1)\n",
    "    plt.tight_layout()\n",
    "#2D plot settings function\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def set_pcolor_properties():\n",
    "    \"\"\"\n",
    "    Sets the best figure properties for pcolormap plot\n",
    "    \"\"\"\n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)  # Set the figure size to 8 inches by 6 inches\n",
    "    plt.rcParams[\"font.size\"] = 20  # Set the font size to 12 points\n",
    "    plt.rcParams[\"axes.linewidth\"] = 1.5  # Set the linewidth of the axes to 1.5\n",
    "    #plt.rcParams[\"axes.grid\"] = True  # Show the grid\n",
    "    #plt.rcParams[\"grid.alpha\"] = 0.5  # Set the transparency of the grid to 0.5\n",
    "    #plt.rcParams[\"grid.linewidth\"] = 0.5  # Set the linewidth of the grid to 0.5\n",
    "    plt.rcParams[\"image.cmap\"] = \"viridis\"  # Set the colormap to viridis\n",
    "    plt.rcParams[\"image.interpolation\"] = \"nearest\"  # Set the interpolation to nearest\n",
    "\n",
    "# create and update folder to save data\n",
    "def create_update_subfolder(base_folder, common_keyword):\n",
    "    common_folder_path = os.path.join(base_folder, common_keyword)\n",
    "    \n",
    "    if os.path.exists(base_folder) and os.path.isdir(common_folder_path):\n",
    "        print(f\"Subfolder '{common_keyword}' already exists within the base folder.\")\n",
    "    else:\n",
    "        os.makedirs(common_folder_path)\n",
    "        print(f\"Subfolder '{common_keyword}' created within the base folder.\")\n",
    "\n",
    "    return common_folder_path\n",
    "\n",
    "# Convert pixel by pixel to qx by qy 2D SAXS plot\n",
    "\n",
    "def convert_saxs_to_q(data, pixel_size, wavelength, detector_distance, beam_x, beam_y):\n",
    "    # Define grid of pixel coordinates (assumes data is square)\n",
    "    #x_pixels,y_pixels = data.shape\n",
    "    x_pixels = 981\n",
    "    y_pixels = 1043\n",
    "    x_coords = np.arange(x_pixels) - beam_x\n",
    "    y_coords = np.arange(y_pixels) - beam_y\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Convert to q-space coordinates\n",
    "    qx = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "\n",
    "    #print (len(xx))\n",
    "    \n",
    "    return qx, qy\n",
    "\n",
    "\n",
    "    #print (qx)\n",
    "    \n",
    "\n",
    "def convert_waxs_to_q(data, pixel_size, wavelength, detector_distance, beam_x, beam_y):\n",
    "    # Define grid of pixel coordinates (assumes data is square)\n",
    "    #x_pixels,y_pixels = data.shape\n",
    "    x_pixels = 487\n",
    "    y_pixels = 195\n",
    "    x_coords = np.arange(x_pixels) - beam_x\n",
    "    y_coords = np.arange(y_pixels) - beam_y\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Convert to q-space coordinates\n",
    "    qx = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * xx / detector_distance)\n",
    "    qy = 1e-9*2 * np.pi / wavelength * np.sin(pixel_size * yy / detector_distance)\n",
    "\n",
    "    print (len(xx))\n",
    "    \n",
    "    return qx, qy\n",
    "\n",
    "\n",
    "def set_plot_style(axs, fonts, xlabel, ylabel):\n",
    "    axs.set_xlabel(xlabel, fontsize=fonts)\n",
    "    axs.set_ylabel(ylabel, fontsize=fonts)\n",
    "    axs.tick_params(axis='both', which='major', direction='out', length=4, width=1)\n",
    "    axs.tick_params(which='minor', width=1, size=2)  # Adjust size as needed\n",
    "    #axs.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "    axs.set_facecolor('white')\n",
    "    axs.spines['top'].set_linewidth(1)\n",
    "    axs.spines['right'].set_linewidth(1)\n",
    "    axs.spines['bottom'].set_linewidth(1)\n",
    "    axs.spines['left'].set_linewidth(1)\n",
    "    axs.tick_params(axis='x', labelsize=fonts)\n",
    "    axs.tick_params(axis='y', labelsize=fonts)\n",
    "\n",
    "    return axs    #print (qx) \n",
    " \n",
    "\n",
    "\n",
    "def darken_colors(num_colors, darker_factor=0.7):\n",
    "    \"\"\"\n",
    "    Darken a set of colors by reducing the value component in the HSV color space.\n",
    "\n",
    "    Parameters:\n",
    "    - num_colors (int): Number of colors to generate.\n",
    "    - darker_factor (float): Factor to control the darkness of the colors. Default is 0.7.\n",
    "\n",
    "    Returns:\n",
    "    - darker_colors (numpy.ndarray): Darkened colors in RGB format.\n",
    "    \"\"\"\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, num_colors))\n",
    "\n",
    "    # Make the colors darker by reducing the value component\n",
    "    darker_colors = colors.copy()\n",
    "\n",
    "    for i in range(num_colors):\n",
    "        rgb = darker_colors[i, :3]  # Extract the RGB values\n",
    "        hsv = plt.cm.colors.rgb_to_hsv(rgb)  # Convert RGB to HSV\n",
    "        hsv[2] *= darker_factor  # Reduce the value component\n",
    "        darker_colors[i, :3] = plt.cm.colors.hsv_to_rgb(hsv)  # Convert back to RGB\n",
    "\n",
    "    return darker_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f319725",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6d758",
   "metadata": {},
   "source": [
    "# Poni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d648378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path_poni_mask = '/Users/akmaurya/Library/CloudStorage/OneDrive-SLACNationalAcceleratorLaboratory/My Onedrive/Data_01/Methanolysis/Python/Data_Reduction/Convert_and_calibrate'\n",
    "#folder_path_poni_mask = r'C:\\Users\\b_tassone\\Desktop\\Anjani\\Python\\Convert_and_calibrate'\n",
    "#folder_path_poni_mask = '/Users/akmaurya/OneDrive - Stanford/Python_library/Acetolysis/May2024/Python/Convert_and_calibrate/Convert_and_calibrate'\n",
    "\n",
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "folder_path_poni_mask = os.path.join(base_dir, 'poni')\n",
    "\n",
    "\n",
    "SAXS_poni_file = \"SAXS_AgBeh_solvent_02.poni\" #loading the AgBeh standard\n",
    "WAXS_poni_file = \"SAXS_AgBeh_solvent_02.poni\" #loading the AgBeh standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998751b",
   "metadata": {},
   "source": [
    "# SAXS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcb7e3",
   "metadata": {},
   "source": [
    "##### I think this remains the same for the toluene runs... double check though (- June 2, 2025 -)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poni_file_path = os.path.join(folder_path_poni_mask,SAXS_poni_file)\n",
    "print(poni_file_path)\n",
    "ai = pyFAI.load(poni_file_path)\n",
    "\n",
    "print(ai)\n",
    "\n",
    "#Define detector parameters for 2D q conversion\n",
    "pixel_size = ai.get_pixel1() # in meters \n",
    "wavelength = ai.get_wavelength() #1.54e-10  # in meters\n",
    "detector_distance = ai.get_dist()  # in meters\n",
    "beam_x=488.787 #from May 2025 Igor file during beamtime\n",
    "beam_y=838.494 # beam center x-coordinate in pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d35b2",
   "metadata": {},
   "source": [
    "#mask for SAXS - June 2, 2025 - need to actually make one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the poni file and extract relevant parameters\n",
    "saxs_mask = os.path.join(folder_path_poni_mask, \"SAXS_solvent_mask_03.edf\")\n",
    "\n",
    "#mask = \"/Users/yuewu/Desktop/8091E/BL15_1M_gcarbon_mask_3.edf\"\n",
    "saxs_mask = fabio.open(saxs_mask).data\n",
    "plt.imshow(saxs_mask)# display the mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6451712",
   "metadata": {},
   "source": [
    "# WAXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the poni file and extract relevant parameters\n",
    "poni_file_path = os.path.join(folder_path_poni_mask, WAXS_poni_file)\n",
    "ai_w = pyFAI.load(poni_file_path)\n",
    "\n",
    "print(ai_w)\n",
    "\n",
    "\n",
    "\n",
    "#Define detector parameters for 2D q conversion\n",
    "pixel_size = ai_w.get_pixel1() # in meters \n",
    "wavelength = ai_w.get_wavelength() #1.54e-10  # in meters\n",
    "detector_distance = ai_w.get_dist()  # in meters\n",
    "beam_x= 668.647\n",
    "beam_y= 105.765 # beam center x-coordinate in pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af82b47",
   "metadata": {},
   "source": [
    "#mask for WAXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the poni file and extract relevant parameters\n",
    "waxs_mask = os.path.join(folder_path_poni_mask, \"SAXS_solvent_mask_03.edf\")\n",
    "\n",
    "#mask = \"/Users/yuewu/Desktop/8091E/BL15_1M_gcarbon_mask_3.edf\"\n",
    "waxs_mask = fabio.open(waxs_mask).data\n",
    "plt.imshow(waxs_mask)# display the mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf818d5",
   "metadata": {},
   "source": [
    "# offsets and air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get this from Empty Beam: shutter closed \n",
    "# is this from one of the first samples?\n",
    "\n",
    "i0_offset = 0\n",
    "bstop_offset = 0\n",
    "\n",
    "# average from without sample in the beam: named as \"Air\" is the standard \n",
    "\n",
    "#May2025: From macro72_20250511_air_pretTolRun3\n",
    "i0_air = 43.138718\n",
    "bstop_air = 45.938285\n",
    "\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a42d54",
   "metadata": {},
   "source": [
    "## function to calculate mu and thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import periodictable as pt\n",
    "import xraydb\n",
    "import numpy as np\n",
    "\n",
    "def calculate_sld_mu_thickness(energy_keV, density, transmission):\n",
    "    try:\n",
    "        # Define PE composition correctly\n",
    "        material = pt.formula(\"C7H8\")  # Changing this to C7H8 for toluene\n",
    "\n",
    "        # Calculate scattering length density (SLD) and mu using periodictable\n",
    "        sld, mu_pt = pt.xray_sld(material, energy=energy_keV, density=density)\n",
    "\n",
    "        # Use xraydb for a more reliable absorption coefficient (mu) calculation\n",
    "        mu_xraydb = xraydb.material_mu(\"C2H4\", energy=energy_keV * 1000, density=density) * 100  # Convert to 1/m\n",
    "\n",
    "        # Calculate thickness using transmission: T = exp(-mu * t) → t = -ln(T) / mu\n",
    "        if transmission <= 0 or transmission > 10:\n",
    "            raise ValueError(\"Transmission must be in the range (0, 1].\")\n",
    "        \n",
    "        thickness = -np.log(transmission) / mu_xraydb  # Thickness in meters\n",
    "\n",
    "        return {\"mu\": mu_xraydb, \"sld\": sld, \"thickness_m\": thickness}\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in calculating SLD, mu, and thickness for PE: {e}\")\n",
    "\n",
    "# Example usage\n",
    "energy_keV = 15      # Energy in keV\n",
    "density_PE = 0.867       # Density in g/cm^3\n",
    "#transmission = 0.92   # Transmission (must be between 0 and 1)\n",
    "\n",
    "#result = calculate_sld_mu_thickness(energy_keV, density_PE, transmission)\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af291f",
   "metadata": {},
   "source": [
    "# Set the directory containing the raw and pdi files and list the keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a7223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def find_folders_with_keyword(folder_path, keyword, skip_folders=None):\n",
    "    folder_names = []\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isdir(item_path) and fnmatch.fnmatch(item, keyword):\n",
    "            if skip_folders is None or item not in skip_folders:\n",
    "                folder_names.append(item)\n",
    "\n",
    "    return folder_names\n",
    "\n",
    "def get_keywords(raw_dir, common_keyword, keyword):\n",
    "    data_folder = os.path.join(raw_dir, common_keyword)\n",
    "    base_SAXS_folder = \"OneD_integrated_SAXS_01/Reduction\"\n",
    "    base_SAXS_folder = os.path.join(raw_dir, base_SAXS_folder)\n",
    "    base_WAXS_folder = \"OneD_integrated_WAXS_01/Reduction\"\n",
    "    base_WAXS_folder = os.path.join(raw_dir, base_WAXS_folder)\n",
    "\n",
    "    SAXS_folder_name = create_update_subfolder(base_SAXS_folder, common_keyword)\n",
    "    WAXS_folder_name = create_update_subfolder(base_WAXS_folder, common_keyword)\n",
    "\n",
    "    folder_names_with_keyword = find_folders_with_keyword(data_folder, keyword)\n",
    "\n",
    "    #ctr_values = [int(name.split('_ctr')[-1]) for name in folder_names_with_keyword]\n",
    "    sorted_folder_names = sorted(folder_names_with_keyword, key=custom_sort)\n",
    "\n",
    "    keywords = sorted_folder_names\n",
    "    return keywords, SAXS_folder_name, WAXS_folder_name,data_folder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cbf96",
   "metadata": {},
   "source": [
    "# read parameters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3489974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_selected_parameters_columns(data_folder, keywords):\n",
    "    # Create an empty dictionary to store the selected columns for each file\n",
    "    selected_columns_dict = {}\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Create a list of file paths that match the file types and current keyword\n",
    "        #raw_file_pattern = f'*{keyword}/SAXS/*.raw'\n",
    "        pdi_file_pattern = f'*{keyword}/*_scan1.csv'\n",
    "        #raw_file_paths = sorted(glob.glob(os.path.join(raw_dir, raw_file_pattern)))\n",
    "        pdi_file_paths = sorted(glob.glob(os.path.join(data_folder, pdi_file_pattern)))\n",
    "\n",
    "        for pdi_file_path in pdi_file_paths:\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(pdi_file_path)\n",
    "                file_name = os.path.splitext(os.path.basename(pdi_file_path))[0]\n",
    "                #print(f\"Reading file: {file_name}\")\n",
    "\n",
    "                # Extract columns 4, 7, 9, and 15\n",
    "                selected_columns = df.iloc[:, [2, 3, 6, 10, 29, 30]]\n",
    "\n",
    "                # Store the selected columns in the dictionary with the file name as the key\n",
    "                selected_columns_dict[file_name] = selected_columns\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {pdi_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return selected_columns_dict\n",
    "\n",
    "# Call the function and pass the raw_dir and keywords as arguments\n",
    "#raw_dir = 'path_to_your_raw_files_directory'\n",
    "#keywords = ['keyword1', 'keyword2', 'keyword3']  # Replace with your actual keywords\n",
    "#\n",
    "# To load the result_dict back from the file, you can use the following code:\n",
    "\n",
    "#print(result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2055127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def read_parameters(data_folder, keyword, result_dict):\n",
    "    \"\"\"\n",
    "    Reads and processes parameter data from CSV files matching a pattern, calculates averages, \n",
    "    and returns a dictionary with results.\n",
    "\n",
    "    Args:\n",
    "        data_folder (str): Path to the folder containing CSV files.\n",
    "        keyword (str): Keyword to filter files.\n",
    "        result_dict (dict): Dictionary containing preloaded DataFrames indexed by filenames.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing averages and filenames.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store results\n",
    "    bstop_dx_avg, ctemp_dx_avg, timer_dx_avg, i0_dx_avg = [], [], [], []\n",
    "    bstop_avg_all, ctemp_avg_all, timer_avg_all, i0_avg_all = [], [], [], []\n",
    "    avg_file_name = []\n",
    "\n",
    "    # File matching pattern\n",
    "    pdi_file_pattern = f'*{keyword}/*_scan1.csv'\n",
    "    pdi_file_paths = sorted(glob.glob(os.path.join(data_folder, pdi_file_pattern)))\n",
    "\n",
    "    for pdi_file_path in pdi_file_paths:\n",
    "        file_name = os.path.splitext(os.path.basename(pdi_file_path))[0]\n",
    "        selected_columns = result_dict.get(file_name)\n",
    "\n",
    "        if selected_columns is not None:\n",
    "            try:\n",
    "                # Access columns safely\n",
    "                i0 = pd.to_numeric(selected_columns.iloc[:, 1], errors='coerce')\n",
    "                bstop = pd.to_numeric(selected_columns.iloc[:, 2], errors='coerce')\n",
    "                ctemp = pd.to_numeric(selected_columns.iloc[:, 4], errors='coerce')\n",
    "                timer = pd.to_numeric(selected_columns.iloc[:, 5], errors='coerce')\n",
    "\n",
    "                # Calculate averages\n",
    "                i0_dx_avg.append(i0.mean(skipna=True))\n",
    "                bstop_dx_avg.append(bstop.mean(skipna=True))\n",
    "                ctemp_dx_avg.append(ctemp.mean(skipna=True))\n",
    "                timer_dx_avg.append(timer.mean(skipna=True))\n",
    "\n",
    "                # Add filename to results\n",
    "                avg_file_name.append(file_name)\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error processing file '{file_name}': {str(e)}\")\n",
    "        else:\n",
    "            print(f\"File '{file_name}' not found in result_dict.\")\n",
    "\n",
    "    # Compute overall averages if data exists\n",
    "    if i0_dx_avg:\n",
    "        i0_avg_all.append(sum(i0_dx_avg) / len(i0_dx_avg))\n",
    "        bstop_avg_all.append(sum(bstop_dx_avg) / len(bstop_dx_avg))\n",
    "        ctemp_avg_all.append(sum(ctemp_dx_avg) / len(ctemp_dx_avg))\n",
    "        timer_avg_all.append(sum(timer_dx_avg) / len(timer_dx_avg))\n",
    "\n",
    "    return {\n",
    "        'bstop_dx_avg': bstop_dx_avg,\n",
    "        'ctemp_dx_avg': ctemp_dx_avg,\n",
    "        'timer_dx_avg': timer_dx_avg,\n",
    "        'i0_dx_avg': i0_dx_avg,\n",
    "        'bstop_avg_all': bstop_avg_all,\n",
    "        'ctemp_avg_all': ctemp_avg_all,\n",
    "        'timer_avg_all': timer_avg_all,\n",
    "        'i0_avg_all': i0_avg_all,\n",
    "        'avg_file_name': avg_file_name,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa0da0",
   "metadata": {},
   "source": [
    "# SAXS Data reduction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "#import warnings\n",
    "\n",
    "# Filter warnings containing the substring \"pyFAI.io\"\n",
    "#warnings.filterwarnings(\"ignore\", message=\"pyFAI.io\")\n",
    "\n",
    "# Map a parameter (e.g., q) to a colormap\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def custom_sort(file_name):\n",
    "    keywords = ['', '', 'HoldT', 'Cooling', 'RampT', '']\n",
    "    \n",
    "    # Find the first matching keyword in file_name\n",
    "    keyword_found = next((kw for kw in keywords if kw in file_name), '')\n",
    "    \n",
    "    # Extract ctr value using regex\n",
    "    ctr_match = re.search(r'ctr(\\d+)', file_name)\n",
    "    ctr_value = int(ctr_match.group(1)) if ctr_match else 0  # Default to 0 if not found\n",
    "\n",
    "    return (keywords.index(keyword_found), ctr_value)\n",
    "\n",
    "\n",
    "def process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, mask, i0_offset, bstop_offset, i0_air, bstop_air):\n",
    "    # print the processting saxs data\n",
    "    print(\"Processing SAXS data...\")\n",
    "\n",
    "    save_SAXS_norm_files = os.path.join(SAXS_folder_name.replace(\"Reduction\", \"\"), \"Correction/Normlised\")\n",
    "    if not os.path.exists(save_SAXS_norm_files):\n",
    "        os.makedirs(save_SAXS_norm_files)\n",
    "    result_dict = process_and_store_selected_parameters_columns(data_folder, keywords)\n",
    "    #warnings.filterwarnings('ignore')\n",
    "    bstop = []\n",
    "    ctemp = []\n",
    "    timer = []\n",
    "    i0 = []\n",
    "    filename_p = []\n",
    "    file_name_norm = []\n",
    "\n",
    "\n",
    "\n",
    "    # Create a figure with 1 row and 2 columns\n",
    "    fig, axs = plt.subplots(figsize=(8,8))\n",
    "    c=0\n",
    "    \n",
    "    num_colors = len(keywords)  # Calculate the number of colors based on the maximum m_key\n",
    "    \n",
    "    print(num_colors)\n",
    "    darker_colors = darken_colors(num_colors, darker_factor=0.7)\n",
    "    for keyword in keywords:\n",
    "        # Create a list of file paths that match the file types and current keyword\n",
    "        raw_file_pattern = f'{keyword}/SAXS/*.raw'\n",
    "        #raw_file_paths = sorted(glob.glob(os.path.join(data_folder, raw_file_pattern)))\n",
    "        raw_file_paths = sorted(glob.glob(os.path.join(data_folder, raw_file_pattern)), key=custom_sort)\n",
    "        #print(raw_file_paths)\n",
    "        #print(keyword)\n",
    "\n",
    "\n",
    "        # read parameters\n",
    "        # Call the function\n",
    "        averages_dict = read_parameters(data_folder, keyword, result_dict)\n",
    "\n",
    "        # Access the averages from the returned dictionary\n",
    "        bstop_avg_all = averages_dict['bstop_avg_all']\n",
    "        ctemp_avg_all = averages_dict['ctemp_avg_all']\n",
    "        timer_avg_all = averages_dict['timer_avg_all']\n",
    "        i0_avg_all = averages_dict['i0_avg_all']\n",
    "        avg_file_name = averages_dict['avg_file_name']\n",
    "\n",
    "\n",
    "        #print(avg_file_name, raw_file_pattern, bstop_avg_all, ctemp_avg_all, timer_avg_all, i0_avg_all)\n",
    "\n",
    "        bstop.append(bstop_avg_all[0])\n",
    "        ctemp.append(ctemp_avg_all[0])\n",
    "        timer.append(timer_avg_all[0])\n",
    "        i0.append(i0_avg_all[0])\n",
    "        filename_p.append(avg_file_name[0])  \n",
    "\n",
    "        #print(i0_offset, bstop_offset)\n",
    "\n",
    "        i0_avg_all[0] = i0_avg_all[0] - i0_offset\n",
    "        bstop_avg_all[0] = bstop_avg_all[0] - bstop_offset\n",
    "\n",
    "        i0_air = i0_air - i0_offset\n",
    "        bstop_air = bstop_air - bstop_offset\n",
    "\n",
    "        trans_factor = (bstop_avg_all[0])#/i0_avg_all[0])/(bstop_air/i0_air)\n",
    "        #trans_factor_t = (bstop_avg_all[0])/i0_avg_all[0]\n",
    "        #print(trans_factor)\n",
    "        normfactor = trans_factor*i0_avg_all[0]\n",
    "\n",
    "        #result = calculate_sld_mu_thickness(energy_keV, density_PE, trans_factor_t)\n",
    "        #thickness = result['thickness_m']\n",
    "        #thickness = 0.0001\n",
    "\n",
    "        #normfactor = trans_factor #* thickness\n",
    "        normfactor = float(normfactor)\n",
    "        #print the transfactor, thicknes and normfactor in good format\n",
    "        print(f\"i0:{i0_avg_all[0]}, bstop: {bstop_avg_all[0]}, normfactor: {normfactor}\")\n",
    "\n",
    "        #normfactor = 1.0\n",
    "        #print(normfactor)\n",
    "\n",
    "        # Initialize the accumulators for the averaged image and pdi data\n",
    "        avg_image = np.zeros((1043, 981))\n",
    "\n",
    "        for i, raw_file_path in enumerate(raw_file_paths):\n",
    "            # Read the raw file\n",
    "            data = np.fromfile(raw_file_path, dtype=np.int32).reshape(1043, 981)\n",
    "            avg_image += data\n",
    "\n",
    "        avg_image /= len(raw_file_paths)\n",
    "\n",
    "        # Assuming raw_file_paths[-1] contains the file path\n",
    "        file_name, file_extension = os.path.splitext(os.path.basename(raw_file_paths[-1]))\n",
    "        file_name = file_name.split('_scan')[0]\n",
    "        file_name = file_name.split('sone_')[1]\n",
    "\n",
    "        # Remove the last 10 characters from the file name and the \".raw\" extension\n",
    "        #file_name = file_name[5:-11]\n",
    "        file_name=f\"{file_name}_all_SAXS.dat\"\n",
    "        file_name_norm.append(file_name)\n",
    "\n",
    "        q, I, error_avg = ai.integrate1d(avg_image, 1000, error_model='poisson', mask=mask, normalization_factor=normfactor, filename=os.path.join(SAXS_folder_name, file_name))\n",
    "        \n",
    "        I_norm = I\n",
    "        I_norm_sigma = error_avg\n",
    "        \n",
    "        axs.loglog(q, I, label=f\"{file_name}\", color=darker_colors[c])\n",
    "        axs.set_xlabel(\"q (nm$^{-1}$)\")\n",
    "        axs.set_ylabel(\"Intensity (a.u.)\")\n",
    "        #axs.set_xlim([0.05, 3])\n",
    "        set_plot_style(axs,20, 'q (nm$^{-1}$)', 'Intensity (a.u.)')\n",
    "        #axs.legend(fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Create a DataFrame for subtracted data\n",
    "        data = {\"q_nm^-1\": q, \"I_avg_nomrlised\": I_norm, \"I_norm_sigma\": I_norm_sigma}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Saving the normalized data to a file\n",
    "        # remove .dat extension\n",
    "        file_name = file_name[:-4]\n",
    "\n",
    "        # remove a string from middle of the file name from _10s to _d and attached first and last part\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dat_filename = f\"{save_SAXS_norm_files}/{file_name}_Norm.dat\"\n",
    "\n",
    "        inst_parameters = f'Timer: {timer_avg_all[0]}, bstop: {bstop_avg_all[0]}, ctemp: {ctemp_avg_all[0]}, I0: {i0_avg_all[0]}'\n",
    "        headers = [\n",
    "            f\"filename: {file_name}\",\n",
    "            f\"background : {''}\",\n",
    "            f'Empty : {\"\"}',\n",
    "            f\"{inst_parameters}\", \n",
    "            'fit_data',\n",
    "            \"q_nm^-1 ------ I_avg_nomrlised ------ I_norm_sigma\"]\n",
    "        commented_headers = ['# ' + header for header in headers]\n",
    "\n",
    "        with open(dat_filename, 'w') as dat_file:\n",
    "            dat_file.write('\\n'.join(commented_headers) + '\\n')\n",
    "            df.to_csv(dat_file, sep='\\t', index=False, header=False)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        c=c+1\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Initialize the result list with the first value as 0\n",
    "    duration = [0]\n",
    "\n",
    "    # Extracting the minimum timestamp from all the timer values\n",
    "    min_timer = min(timer)\n",
    "    #print(timer)\n",
    "\n",
    "    # Subtract the minimum timestamp from each timer value and append to the result list\n",
    "    for i in range(1, len(timer)):\n",
    "        duration_avg = (timer[i] - min_timer) / 3600\n",
    "        duration.append(duration_avg)\n",
    "    \n",
    "    # save the filename, duration, bstop, ctemp, i0 values in a xlsx file\n",
    "   \n",
    "    df = pd.DataFrame(list(zip(file_name_norm, timer, bstop, ctemp, i0)), columns =['Filename', 'Timer', 'Bstop', 'Ctemp', 'I0'])\n",
    "    df.to_excel(os.path.join(SAXS_folder_name, 'SAXS_parameters_M12.xlsx'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a figure and subplots\n",
    "    fig, ((axs1,axs2), (axs3, axs4), (axs5, axs6)) = plt.subplots(3,2, figsize=(10, 12))\n",
    "\n",
    "\n",
    "    # Plot data on the first subplot\n",
    "    axs1.plot(duration, bstop, 'o', color='b', label='bstop')\n",
    "    #axs1b = axs1.twinx()\n",
    "    axs1.plot(duration, i0, 'o', color='r', label='I0')\n",
    "    #axs1.grid(True)\n",
    "    axs1.legend()\n",
    "    #axs1b.legend()\n",
    "    axs1.set_xlabel('Duration [hr]')\n",
    "    axs1.set_ylabel('Bstop and I0')\n",
    "    #axs1b.set_ylabel('I0')\n",
    "    set_plot_style(axs1,20, 'Duration', 'Bstop')\n",
    "    #set_plot_style(axs1b,20, 'Duration', 'I0')\n",
    "    \n",
    "\n",
    "    # Plot data on the second subplot\n",
    "    axs2.plot(duration, ctemp, 'o', color='r', label='ctemp')\n",
    "    #axs2.grid(True)\n",
    "    axs2.legend()\n",
    "    axs2.set_xlabel('Duration')\n",
    "    axs2.set_ylabel('Ctemp')\n",
    "    set_plot_style(axs2,20, 'Duration [hr]', 'Ctemp [C]')\n",
    "\n",
    "    #Creating subplots with just bstop and i0 plots\n",
    "\n",
    "    axs3.plot(duration, i0, 'o', color='r', label='I0')\n",
    "    axs3.legend()\n",
    "    axs3.set_xlabel('Duration [hr]')\n",
    "    axs3.set_ylabel('I0')\n",
    "    set_plot_style(axs3,15, 'Duration [hr]', 'I0')\n",
    "\n",
    "    axs4.plot(duration, bstop, 'o', color='b', label='bstop')\n",
    "    axs4.legend()\n",
    "    axs4.set_xlabel('Duration [hr]')\n",
    "    axs4.set_ylabel('Bstop')\n",
    "    set_plot_style(axs4,15, 'Duration [hr]', 'Bstop')\n",
    "\n",
    "    #Double y axis plot for duration, bstop, and i0\n",
    "    line5 = axs5.plot(duration, bstop, 'o', color='b', label='bstop')\n",
    "    axs5b = axs5.twinx()\n",
    "    line5b = axs5b.plot(duration, i0, 'o', color='r', label='I0')\n",
    "    lines5 = line5 + line5b\n",
    "    labs5 = [l.get_label() for l in lines5]\n",
    "    axs5.legend(lines5, labs5, loc=0)\n",
    "    axs5.set_xlabel('Duration [hr]')\n",
    "    axs5.set_ylabel('Bstop')\n",
    "    axs5b.set_ylabel('I0')\n",
    "    set_plot_style(axs5,15, 'Duration', 'bstop')\n",
    "    set_plot_style(axs5b,15, 'Duration', 'I0')\n",
    "\n",
    "    #Double y axis plot for temperature, bstop, and i0\n",
    "    line6 = axs6.plot(ctemp, bstop, 'o', color='b', label='bstop')\n",
    "    axs6b = axs6.twinx()\n",
    "    line6b = axs6b.plot(ctemp, i0, 'o', color='r', label='I0')\n",
    "    lines6 = line6 + line6b\n",
    "    labs6 = [l.get_label() for l in lines6]\n",
    "    axs6.legend(lines6, labs6, loc=0)\n",
    "    axs6.set_xlabel('Temperature [C]')\n",
    "    axs6.set_ylabel('Bstop')\n",
    "    axs6b.set_ylabel('I0')\n",
    "    set_plot_style(axs6,15, 'Temp', 'Bstop')\n",
    "    set_plot_style(axs6b,15, 'Temp', 'I0')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#raw_dir_aceto = '/Users/akmaurya/OneDrive - Stanford/Data_01/Acetolysis/May2024/'\n",
    "#raw_dir_meth = 'C:/Users/akmaurya/OneDrive - Stanford/Data_01/Methanolysis/May2024/'\n",
    "\n",
    "raw_dir = r'X:\\bl1-5\\Anjani\\Autoxidation\\Jan2025\\atT'\n",
    "\n",
    "\n",
    "# Run1\n",
    "common_keyword = \"\"\n",
    "keyword = \"Run5*\"\n",
    "\n",
    "keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\"\"\"\n",
    "print(\"Keywords:\")\n",
    "for kw in keywords:\n",
    "    print(f\"    '{kw}',\")\n",
    "\"\"\"\n",
    "\n",
    "#process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai)\n",
    "#process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c9b9c",
   "metadata": {},
   "source": [
    "# WAXS Data reduction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeb486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Filter warnings containing the substring \"pyFAI.io\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"pyFAI.io\")\n",
    "\n",
    "# Your plotting code here...\n",
    "def custom_sort(file_name):\n",
    "    keywords = ['ctr', 'RampT', 'HoldT', 'Cooling', '']\n",
    "    keyword_found = next((kw for kw in keywords if kw in file_name), '')\n",
    "    ctr_value = int(file_name.split('ctr')[-1].split('_')[0]) if 'ctr' in file_name else 0\n",
    "    return (keywords.index(keyword_found), ctr_value)\n",
    "\n",
    "def process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask, i0_offset, bstop_offset, i0_air, bstop_air):\n",
    "    # print the processting waxs data\n",
    "    print(\"Processing WAXS data...\")\n",
    "    save_SAXS_norm_files = os.path.join(WAXS_folder_name.replace(\"Reduction\", \"\"), \"Correction/Normlised\")\n",
    "    if not os.path.exists(save_SAXS_norm_files):\n",
    "        os.makedirs(save_SAXS_norm_files)\n",
    "\n",
    "    result_dict = process_and_store_selected_parameters_columns(data_folder, keywords)\n",
    "    #warnings.filterwarnings('ignore')\n",
    "            # Create a figure with 1 row and 2 columns\n",
    "    fig, axs = plt.subplots(figsize=(12,12))\n",
    "    \n",
    "    bstop = []\n",
    "    ctemp = []\n",
    "    timer = []\n",
    "    i0 = []\n",
    "    filename_p = []\n",
    "    file_name_norm = []\n",
    "\n",
    "    c=0 \n",
    "    \n",
    "    num_colors = len(keywords)\n",
    "    \n",
    "    darker_colors = darken_colors(num_colors, darker_factor=0.7)\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Create a list of file paths that match the file types and current keyword\n",
    "        raw_file_pattern = f'{keyword}/WAXS/*.raw'\n",
    "        #raw_file_paths = sorted(glob.glob(os.path.join(data_folder, raw_file_pattern)))\n",
    "        raw_file_paths = sorted(glob.glob(os.path.join(data_folder, raw_file_pattern)), key=custom_sort)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "\n",
    "        averages_dict = read_parameters(data_folder, keyword, result_dict)\n",
    "\n",
    "        # Access the averages from the returned dictionary\n",
    "        bstop_avg_all = averages_dict['bstop_avg_all']\n",
    "        ctemp_avg_all = averages_dict['ctemp_avg_all']\n",
    "        timer_avg_all = averages_dict['timer_avg_all']\n",
    "        i0_avg_all = averages_dict['i0_avg_all']\n",
    "        avg_file_name = averages_dict['avg_file_name']\n",
    "\n",
    "        \n",
    "\n",
    "        #print(avg_file_name, raw_file_pattern, bstop_avg_all, ctemp_avg_all, timer_avg_all, i0_avg_all)\n",
    "\n",
    "        bstop.append(bstop_avg_all[0])\n",
    "        ctemp.append(ctemp_avg_all[0])\n",
    "        timer.append(timer_avg_all[0])\n",
    "        i0.append(i0_avg_all[0])\n",
    "        filename_p.append(avg_file_name[0]) \n",
    "\n",
    "        i0_avg_all[0] = i0_avg_all[0] - i0_offset\n",
    "        bstop_avg_all[0] = bstop_avg_all[0] - bstop_offset\n",
    "\n",
    "        i0_air = i0_air - i0_offset\n",
    "        bstop_air = bstop_air - bstop_offset\n",
    "\n",
    "        trans_factor = (bstop_avg_all[0])#/i0_avg_all[0])/(bstop_air/i0_air)\n",
    "        trans_factor_t = (bstop_avg_all[0])/i0_avg_all[0]\n",
    "        #print(trans_factor)\n",
    "        trans_factor = trans_factor*i0_avg_all[0]\n",
    "\n",
    "        result = calculate_sld_mu_thickness(energy_keV, density_PE, trans_factor_t)\n",
    "        thickness = result['thickness_m']\n",
    "\n",
    "        normfactor = trans_factor #* thickness\n",
    "        normfactor = float(normfactor)\n",
    "        #print the transfactor, thicknes and normfactor in good format\n",
    "        print(f\"i0:{i0_avg_all[0]}, bstop: {bstop_avg_all[0]}, transmission factor: {trans_factor_t}, thickness: {thickness}, normfactor: {normfactor}\")\n",
    "\n",
    "        normfactor = float(normfactor)\n",
    "        #normfactor = 1.0\n",
    "        #print(normfactor)\n",
    "\n",
    "\n",
    "        # Initialize the accumulators for the averaged image and pdi data\n",
    "        avg_image = np.zeros((195, 487))\n",
    "\n",
    "        for i, raw_file_path in enumerate(raw_file_paths):\n",
    "            # Read the raw file\n",
    "            data = np.fromfile(raw_file_path, dtype=np.int32).reshape(195, 487)\n",
    "            avg_image += data\n",
    "\n",
    "            file_name = os.path.splitext(os.path.basename(raw_file_path))[0]\n",
    "\n",
    "        avg_image /= len(raw_file_paths)\n",
    "\n",
    "        # Assuming raw_file_paths[-1] contains the file path\n",
    "        file_name, file_extension = os.path.splitext(os.path.basename(raw_file_paths[-1]))\n",
    "\n",
    "        # Remove the last 10 characters from the file name and the \".raw\" extension\n",
    "        # split the file name _scan and remove the last part of the file name\n",
    "        file_name = file_name.split('_scan')[0]\n",
    "        file_name=f\"{file_name}_all_WAXS.dat\"\n",
    "        file_name=file_name.split('b_tassone_')[1]\n",
    "        file_name_norm.append(file_name)\n",
    "\n",
    "\n",
    "\n",
    "        q, I, error_avg = ai_w.integrate1d(avg_image, 1000,error_model='poisson', mask=mask,normalization_factor=normfactor,filename=os.path.join(WAXS_folder_name, file_name))\n",
    "   \n",
    "        I_norm = I\n",
    "        I_norm_sigma = error_avg\n",
    "\n",
    "        axs.plot(q, I, '-', label=f\"{file_name}\",color=darker_colors[c])\n",
    "        axs.set_xlabel(\"q (nm$^{-1}$)\")\n",
    "        axs.set_ylabel(\"Intensity (a.u.)\")\n",
    "        set_plot_style(axs,20, 'q (nm$^{-1}$)', 'Intensity (a.u.)')\n",
    "        #axs.set_xlim([0.05, 3])\n",
    "        #axs.legend(fontsize=12)\n",
    "\n",
    "        # Create a DataFrame for subtracted data\n",
    "        data = {\"q_nm^-1\": q, \"I_avg_nomrlised\": I_norm, \"I_norm_sigma\": I_norm_sigma}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Saving the normalized data to a file\n",
    "        # remove .dat extension\n",
    "        file_name = file_name[:-4]\n",
    "        dat_filename = f\"{save_SAXS_norm_files}/{file_name}_Norm.dat\"\n",
    "\n",
    "        inst_parameters = f'Timer: {timer_avg_all[0]}, bstop: {bstop_avg_all[0]}, ctemp: {ctemp_avg_all[0]}, I0: {i0_avg_all[0]}'\n",
    "        headers = [\n",
    "            f\"filename: {file_name}\",\n",
    "            f\"background : {''}\",\n",
    "            f'Empty : {\"\"}',\n",
    "            f\"{inst_parameters}\", \n",
    "            'fit_data',\n",
    "            \"q_nm^-1 ------ I_avg_nomrlised ------ I_norm_sigma\"]\n",
    "        commented_headers = ['# ' + header for header in headers]\n",
    "\n",
    "        with open(dat_filename, 'w') as dat_file:\n",
    "            dat_file.write('\\n'.join(commented_headers) + '\\n')\n",
    "            df.to_csv(dat_file, sep='\\t', index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "        c = c+1\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Initialize the result list with the first value as 0\n",
    "    duration = [0]\n",
    "\n",
    "    # Extracting the minimum timestamp from all the timer values\n",
    "    min_timer = min(timer)\n",
    "    #print(timer)\n",
    "\n",
    "    # Subtract the minimum timestamp from each timer value and append to the result list\n",
    "    for i in range(1, len(timer)):\n",
    "        duration_avg = (timer[i] - min_timer) / 3600\n",
    "        duration.append(duration_avg)\n",
    "    # save the filename, duration, bstop, ctemp, i0 values in a xlsx file\n",
    "   \n",
    "    df = pd.DataFrame(list(zip(file_name_norm, timer, bstop, ctemp, i0)), columns =['Filename', 'Timer', 'Bstop', 'Ctemp', 'I0'])\n",
    "    df.to_excel(os.path.join(WAXS_folder_name, 'WAXS_parameters.xlsx'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Create a figure and subplots\n",
    "    fig, (axs1,axs2) = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "    # Plot data on the first subplot\n",
    "    axs1.plot(duration, bstop, 'o', color='b', label='bstop')\n",
    "    axs1.plot(duration, i0, 'o', color='r', label='I0')\n",
    "    #axs1.grid(True)\n",
    "    axs1.legend()\n",
    "    axs1.set_xlabel('Duration [hr]')\n",
    "    axs1.set_ylabel('Bstop and I0')\n",
    "    set_plot_style(axs1,20, 'Duration', 'Bstop, I0')\n",
    "    \n",
    "\n",
    "    # Plot data on the second subplot\n",
    "    axs2.plot(duration, ctemp, 'o', color='r', label='ctemp')\n",
    "    #axs2.grid(True)\n",
    "    axs2.legend()\n",
    "    axs2.set_xlabel('Duration')\n",
    "    axs2.set_ylabel('Ctemp')\n",
    "    set_plot_style(axs2,20, 'Duration [hr]', 'Ctemp [C]')\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4ee8e",
   "metadata": {},
   "source": [
    "# Ploting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def darken_colors(num_colors, darker_factor=0.7):\n",
    "    \"\"\"\n",
    "    Darken a set of colors by reducing the value component in the HSV color space.\n",
    "\n",
    "    Parameters:\n",
    "    - num_colors (int): Number of colors to generate.\n",
    "    - darker_factor (float): Factor to control the darkness of the colors. Default is 0.7.\n",
    "\n",
    "    Returns:\n",
    "    - darker_colors (numpy.ndarray): Darkened colors in RGB format.\n",
    "    \"\"\"\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, num_colors))\n",
    "\n",
    "    # Make the colors darker by reducing the value component\n",
    "    darker_colors = colors.copy()\n",
    "\n",
    "    for i in range(num_colors):\n",
    "        rgb = darker_colors[i, :3]  # Extract the RGB values\n",
    "        hsv = plt.cm.colors.rgb_to_hsv(rgb)  # Convert RGB to HSV\n",
    "        hsv[2] *= darker_factor  # Reduce the value component\n",
    "        darker_colors[i, :3] = plt.cm.colors.hsv_to_rgb(hsv)  # Convert back to RGB\n",
    "\n",
    "    return darker_colors\n",
    "\n",
    "def read_data_files(folder_path, keywords):\n",
    "    data_list = []\n",
    "    m_key = 0  # Initialize m_key to 0\n",
    "    m_keys = []  # Create a list to store m_key for each file\n",
    "    legend = []  # Create a list to store legends\n",
    "\n",
    "    def sort_key(filename):\n",
    "        ctr_index = filename.find(\"ctr\")\n",
    "        if ctr_index != -1:\n",
    "            numeric_part = filename[ctr_index + 3:]\n",
    "            numeric_part = numeric_part.split('_')[0]  # Extract numeric part before the next underscore\n",
    "            try:\n",
    "                return int(numeric_part)\n",
    "            except ValueError:\n",
    "                return float('inf')\n",
    "        else:\n",
    "            return float('inf')\n",
    "\n",
    "    for keyword in keywords:\n",
    "        files = glob.glob(os.path.join(folder_path, f\"*{keyword}*.dat\"))\n",
    "        files = sorted(files, key=lambda x: (sort_key(x), x))\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                start_index = None\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.strip().startswith(\"#\"):\n",
    "                        continue\n",
    "                    else:\n",
    "                        start_index = i\n",
    "                        break\n",
    "\n",
    "                if start_index is not None:\n",
    "                    df = pd.read_csv(file, delim_whitespace=True, skiprows=start_index, header=None,\n",
    "                                     names=[\"q_nm^-1\", \"I\", 'sigma', ])\n",
    "                    df[\"File\"] = file\n",
    "                    if not df.empty:\n",
    "                        data_list.append(df)\n",
    "\n",
    "                        # Extract temperature from the filename\n",
    "                        # temperature = file.split('_T')[1].split('_')[0]\n",
    "\n",
    "                        # Print only the file name (without path) just before plotting\n",
    "                        file_name = os.path.basename(file)\n",
    "                        #print(f\" {file_name}\")\n",
    "\n",
    "                        # Check if temperature is present in the filename\n",
    "\n",
    "                        legend.append(file_name)\n",
    "\n",
    "                        m_keys.append(m_key)  # Append m_key for this file to the list\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file}: {str(e)}\")\n",
    "            m_key += 1  # Increment m_key for each file\n",
    "        m_key += 1  # Increment m_key by 2 when switching keywords\n",
    "\n",
    "    # After the loop, concatenate the DataFrames\n",
    "    data = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # print(legend)\n",
    "    return data, m_keys, legend\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_saxs_waxs_plot(folder_path_saxs, folder_path_waxs, keywords, m_keys, data_saxs, data_waxs, legend_saxs, legend_waxs):\n",
    "    # Create a 1x2 grid of subplots\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fonts = 20\n",
    "    \n",
    "    num_colors = max(m_keys) + 1  # Calculate the number of colors based on the maximum m_key\n",
    "    darker_colors = darken_colors(num_colors, darker_factor=0.7)\n",
    "    \n",
    "    #print(keywords)\n",
    "    \n",
    "    # SAXS plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('SAXS',fontsize=fonts)\n",
    "    for i, file in enumerate(data_saxs[\"File\"].unique()):\n",
    "        subset = data_saxs[data_saxs[\"File\"] == file]\n",
    "        qmin, qmax = 0.07, 0.8\n",
    "        subset = subset[(subset[\"q_nm^-1\"] >= qmin) & (subset[\"q_nm^-1\"] <= qmax)]\n",
    "        \n",
    "        # Check if any data points are present before plotting\n",
    "        if not subset.empty:\n",
    "            #print(file)\n",
    "            #print(subset)\n",
    "            plt.loglog(subset[\"q_nm^-1\"], subset[\"I\"] * (1 ** m_keys[i]), markersize=0.5, color=darker_colors[m_keys[i]],label=legend_saxs[i].replace('_all_SAXS.dat', '') if legend_saxs is not None else None)\n",
    "\n",
    "    plt.xlim(0.07, 0.8)\n",
    "    plt.xlabel('q [$\\\\mathrm{nm^{-1}}$]', fontsize=fonts)\n",
    "    plt.ylabel(\"Intensity [a.u.]\", fontsize=fonts)\n",
    "    plt.xticks(fontsize=fonts)  # Set x-axis tick to 20\n",
    "    plt.yticks(fontsize=fonts)  # Set y-axis tick to 20\n",
    "    \n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.gca().spines['top'].set_linewidth(1)\n",
    "    plt.gca().spines['right'].set_linewidth(1)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1)\n",
    "    plt.gca().spines['left'].set_linewidth(1)\n",
    "    plt.tick_params(axis='both', which='both', direction='out', length=4, width=1)\n",
    "\n",
    "    #plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=10, frameon=False)\n",
    "    #plt.legend(loc='upper right', fontsize=10, frameon=True)\n",
    "\n",
    "    # WAXS plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('WAXS',fontsize=fonts)\n",
    "    for i, file in enumerate(data_waxs[\"File\"].unique()):\n",
    "        subset = data_waxs[data_waxs[\"File\"] == file]\n",
    "        \n",
    "        qmin, qmax = 10, 40\n",
    "        subset = subset[(subset[\"q_nm^-1\"] >= qmin) & (subset[\"q_nm^-1\"] <= qmax)]\n",
    "        \n",
    "        \n",
    "        plt.plot(subset[\"q_nm^-1\"], subset[\"I\"] + (0* m_keys[i]), markersize=0.5,color=darker_colors[m_keys[i]], label=legend_waxs[i].replace('_all_WAXS.dat', '') if legend_waxs is not None else None)\n",
    "\n",
    "    #plt.xlim(10, 25)\n",
    "    plt.xlabel('q [$\\\\mathrm{nm^{-1}}$]', fontsize=fonts)\n",
    "    plt.ylabel(\"Intensity [a.u.]\", fontsize=fonts)\n",
    "    plt.xticks(fontsize=fonts)  # Set x-axis tick to 20\n",
    "    plt.yticks(fontsize=fonts)  # Set y-axis tick to 20\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.gca().spines['top'].set_linewidth(1)\n",
    "    plt.gca().spines['right'].set_linewidth(1)\n",
    "    plt.gca().spines['bottom'].set_linewidth(1)\n",
    "    plt.gca().spines['left'].set_linewidth(1)\n",
    "    plt.tick_params(axis='both', which='both', direction='out', length=4, width=1)\n",
    "\n",
    "    #plt.legend(loc='center left', bbox_to_anchor=(1.1, 0.5), fontsize=8, frameon=False)\n",
    "\n",
    "    \n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################### save figure\n",
    "    # Use the first 30 characters of the first keyword as the figure name\n",
    "    figure_name = keywords[0][:30].replace('*', '')\n",
    "    \n",
    "    \n",
    "    # Save the figure to the specified folder path with the given name\n",
    "    #plt.savefig(f\"{folder_path}/{figure_name}.png\", dpi=300)\n",
    "    \n",
    "    \n",
    "    # Create the \"Figure\" folder if it doesn't exist\n",
    "    #figure_folder = os.path.join(folder_path_saxs, 'Figure_SAXS_WAXS')\n",
    "    #os.makedirs(figure_folder, exist_ok=True)\n",
    "    figure_folder = os.path.join(os.path.dirname(folder_path_saxs), 'Figure_saxs_waxs_Reduced')\n",
    "    os.makedirs(figure_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Save the figure in the specified folder\n",
    "    figure_path = os.path.join(figure_folder, f'{figure_name}_SAXS_WAXS.png')\n",
    "    plt.savefig(figure_path, dpi=300)\n",
    "\n",
    "    ###################\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "def plot_saxs_waxs_data(all_keywords, Run_number):\n",
    "    #folder_path_saxs_base = r'X:\\bl1-5\\Anjani\\Autoxidation\\Jan2025\\RT\\RTSoak\\OneD_integrated_SAXS_01\\Reduction'\n",
    "    #folder_path_waxs_base = r'X:\\bl1-5\\Anjani\\Autoxidation\\Jan2025\\RT\\RTSoak\\OneD_integrated_WAXS_01\\Reduction'\n",
    "    folder_path_saxs_base = r'X:\\bl1-5\\Alexis\\2025\\202505\\20250501\\OneD_integrated_SAXS_01\\Reduction'\n",
    "    folder_path_waxs_base = r'X:\\bl1-5\\Alexis\\2025\\202505\\20250501\\OneD_integrated_WAXS_01\\Reduction'\n",
    "\n",
    "    for keywords in all_keywords:\n",
    "        # Read SAXS data\n",
    "        result_data_saxs, m_keys_saxs, legend_saxs = read_data_files(folder_path_saxs_base, keywords)\n",
    "\n",
    "        # Read WAXS data\n",
    "        result_data_waxs, m_keys_waxs, legend_waxs = read_data_files(folder_path_waxs_base, keywords)\n",
    "\n",
    "        # Create SAXS-WAXS combined plot\n",
    "        create_saxs_waxs_plot(\n",
    "            folder_path_saxs_base,\n",
    "            folder_path_waxs_base,\n",
    "            keywords,\n",
    "            m_keys_saxs,\n",
    "            result_data_saxs,\n",
    "            result_data_waxs,\n",
    "            legend_saxs,\n",
    "            legend_waxs,\n",
    "        )\n",
    "# https://pubs.rsc.org/en/content/articlelanding/2021/gc/d0gc03536j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eef2e3",
   "metadata": {},
   "source": [
    "# Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ce98d",
   "metadata": {},
   "source": [
    "## Dark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b505f68",
   "metadata": {},
   "source": [
    "Updating this dark to be for May 11th - using darks within macro79_CapA_toluene_SBA-12nm_10wtpt_run3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "\n",
    "raw_dir = os.path.join(base_dir,\"macro79_CapA_toluene_SBA-12nm_10wtpt_run3\")\n",
    "\n",
    "#raw_dir_meth = r'X:\\bl1-5\\Anjani\\RedesignedPlastic\\Jan2025\\Fibers3'\n",
    "\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = [\"*dark*\" ]# file name\n",
    "\n",
    "#keep these as zero for the dark - what ANJANI says.....\n",
    "i0_offset = 0\n",
    "bstop_offset = 0 \n",
    "\n",
    "# loop over the keywords\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, waxs_mask, i0_offset, bstop_offset, i0_air, bstop_air)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e336ddf",
   "metadata": {},
   "source": [
    "## M79 SBA-15 / Toluene run 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865433e",
   "metadata": {},
   "source": [
    "### Dark for run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58788b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "\n",
    "raw_dir = os.path.join(base_dir, 'macro79_CapA_toluene_SBA-12nm_10wtpt_run3')\n",
    "\n",
    "i0_offset = 0\n",
    "bstop_offset = 0\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M79*dark*']# file name\n",
    "\n",
    "result = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "     print(f\"Keywords: {keywords}, SAXS_folder_name: {SAXS_folder_name}, WAXS_folder_name: {WAXS_folder_name}, data_folder: {data_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5029b",
   "metadata": {},
   "source": [
    "### SBA15 X Toluene run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "\n",
    "raw_dir = os.path.join(base_dir, 'macro79_CapA_toluene_SBA-12nm_10wtpt_run3')\n",
    "\n",
    "i0_offset = (2.397306 + 2.214361) / 2\n",
    "bstop_offset = (0.530458 + 0.544308) / 2\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M79*capA*']# file name\n",
    "\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "     # Function to extract the number after 'ctr'\n",
    "     #def extract_ctr_value(s):\n",
    "          #match = re.search(r'ctr(\\d+)', s)\n",
    "          #return int(match.group(1)) if match else -1  # fallback to -1 if no match\n",
    "\n",
    "     # Sort the keywords list based on the ctr number\n",
    "     #keywords.sort(key=extract_ctr_value)\n",
    "\n",
    "     keywords = keywords[4:]\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     #Excluding the first 10 items from the keywords list (since the experiment starts around ctr 9)\n",
    "     #print(f\"keywords are {keywords}\")\n",
    "\n",
    "\n",
    "     #process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro79_CapA_toluene_SBA-12nm_10wtpt_run3')\n",
    "\n",
    "i0_offset = (2.397306 + 2.214361) / 2 #dark values, averaged\n",
    "bstop_offset = (0.530458 + 0.544308) / 2 #dark values, averaged\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M79*capA*']# file name\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "     # Function to extract the number after 'ctr'\n",
    "     def extract_ctr_value(s):\n",
    "          match = re.search(r'ctr(\\d+)', s)\n",
    "          return int(match.group(1)) if match else -1  # fallback to -1 if no match\n",
    "\n",
    "     # Sort the keywords list based on the ctr number\n",
    "     keywords.sort(key=extract_ctr_value)\n",
    "\n",
    "     #keywords = keywords[1:]\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     #Excluding the first 10 items from the keywords list (since the experiment starts around ctr 9)\n",
    "     #print(f\"keywords are {keywords}\")\n",
    "\n",
    "\n",
    "     #process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6db63",
   "metadata": {},
   "source": [
    "### Pure toluene, run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "\n",
    "raw_dir = os.path.join(base_dir, 'macro77_20250511_CapA_toluene')\n",
    "\n",
    "print(raw_dir)\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = [\"*capA*\" ]# file name\n",
    "\n",
    "\n",
    "i0_offset = 2.397306 # intial dark values from run 2\n",
    "bstop_offset = 0.530458 # intial dark values from run 2\n",
    "\n",
    "# loop over the keywords\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, waxs_mask, i0_offset, bstop_offset, i0_air, bstop_air)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c896d",
   "metadata": {},
   "source": [
    "### M76 glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0eb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "\n",
    "raw_dir = os.path.join(base_dir, 'macro76_20250511_CapA_empty')\n",
    "\n",
    "print(raw_dir)\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = [\"*capA*\" ]# file name\n",
    "\n",
    "\n",
    "i0_offset = 2.397306 # intial dark values from run 2\n",
    "bstop_offset = 0.530458 # intial dark values from run 2\n",
    "\n",
    "# loop over the keywords\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, waxs_mask, i0_offset, bstop_offset, i0_air, bstop_air)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b61513",
   "metadata": {},
   "source": [
    "## M83 SBA-15 / Toluene run 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01344580",
   "metadata": {},
   "source": [
    "### Dark for M83 - SBA 12 nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro83_CapA_toluene_SBA-12nm_10wtpt_run4')\n",
    "\n",
    "i0_offset = 0\n",
    "bstop_offset = 0\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M83*dark*']# file name\n",
    "\n",
    "result = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "     print(f\"Keywords: {keywords}, SAXS_folder_name: {SAXS_folder_name}, WAXS_folder_name: {WAXS_folder_name}, data_folder: {data_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ce20e",
   "metadata": {},
   "source": [
    "### SBA15 X Toluene run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae81308",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro83_CapA_toluene_SBA-12nm_10wtpt_run4')\n",
    "\n",
    "i0_offset = (2.434937 + 2.276866) / 2\n",
    "bstop_offset = (0.549984 + 0.55664) / 2\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M83*capA*']# file name\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "     # Function to extract the number after 'ctr'\n",
    "     def extract_ctr_value(s):\n",
    "          match = re.search(r'ctr(\\d+)', s)\n",
    "          return int(match.group(1)) if match else -1  # fallback to -1 if no match\n",
    "\n",
    "     # Sort the keywords list based on the ctr number\n",
    "     keywords.sort(key=extract_ctr_value)\n",
    "\n",
    "     keywords = keywords[1:35]\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     #Excluding the first 10 items from the keywords list (since the experiment starts around ctr 9)\n",
    "     #print(f\"keywords are {keywords}\")\n",
    "\n",
    "\n",
    "     #process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c89eb9",
   "metadata": {},
   "source": [
    "### pure toluene, run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "base_dir = r'/Users/alexisvoulgaropoulos/Library/CloudStorage/OneDrive-Stanford/BOTTLE/BEAMTIME/May2025_BL1-5_data/20250501/'\n",
    "\n",
    "raw_dir = os.path.join(base_dir, 'macro82_20250512_CapA_toluene')\n",
    "\n",
    "print(raw_dir)\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = [\"*capA*\" ]# file name\n",
    "\n",
    "\n",
    "i0_offset = 2.434937 # intial dark values from run 2\n",
    "bstop_offset = 0.549984 # intial dark values from run 2\n",
    "\n",
    "# loop over the keywords\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, waxs_mask, i0_offset, bstop_offset, i0_air, bstop_air)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f5fc9",
   "metadata": {},
   "source": [
    "## M43 SBA15 12 nm run 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff6fd6",
   "metadata": {},
   "source": [
    "### Dark for M43 - SBA 12 nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro36_20250504_capA_SBA12nm_capG_HDPE-SBA12nm_run3')\n",
    "\n",
    "i0_offset = 0\n",
    "bstop_offset = 0\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M36*dark*']# file name\n",
    "\n",
    "result = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "     print(f\"Keywords: {keywords}, SAXS_folder_name: {SAXS_folder_name}, WAXS_folder_name: {WAXS_folder_name}, data_folder: {data_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c636188",
   "metadata": {},
   "source": [
    "### SBA15 X HDPE run 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e16693",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro43_20250508_capD_HDPE_capH_HDPE-SBA12nm_run4')\n",
    "\n",
    "i0_offset = (2.672296 + 2.025881) / 2\n",
    "bstop_offset = (0.562345 + 0.553299) / 2\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M43*capH_HDPE*']# file name\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "     # Function to extract the number after 'ctr'\n",
    "     def extract_ctr_value(s):\n",
    "          match = re.search(r'ctr(\\d+)', s)\n",
    "          return int(match.group(1)) if match else -1  # fallback to -1 if no match\n",
    "\n",
    "     # Sort the keywords list based on the ctr number\n",
    "     keywords.sort(key=extract_ctr_value)\n",
    "\n",
    "     keywords = keywords[1:20]\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     #Excluding the first 10 items from the keywords list (since the experiment starts around ctr 9)\n",
    "     #print(f\"keywords are {keywords}\")\n",
    "\n",
    "\n",
    "     #process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5df9b",
   "metadata": {},
   "source": [
    "## M75 10 wt% SBA-15 in Toluene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3cb67",
   "metadata": {},
   "source": [
    "### Dark for M75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro75_CapA_toluene_SBA-12nm_10wtpt_run2')\n",
    "\n",
    "i0_offset = 0\n",
    "bstop_offset = 0\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M75*dark*']# file name\n",
    "\n",
    "result = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "     print(f\"Keywords: {keywords}, SAXS_folder_name: {SAXS_folder_name}, WAXS_folder_name: {WAXS_folder_name}, data_folder: {data_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc71fa",
   "metadata": {},
   "source": [
    "### 10 wt% SBA in Toluene run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = os.path.join(base_dir, 'macro75_CapA_toluene_SBA-12nm_10wtpt_run2')\n",
    "\n",
    "i0_offset = (2.672296 + 2.025881) / 2\n",
    "bstop_offset = (0.562345 + 0.553299) / 2\n",
    "\n",
    "common_keyword = \"\"  # folder name\n",
    "keywords = ['*M75*capA_SBA12*']# file name\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "     keywords, SAXS_folder_name, WAXS_folder_name,data_folder = get_keywords(raw_dir, common_keyword, keyword)\n",
    "\n",
    "     # Function to extract the number after 'ctr'\n",
    "     def extract_ctr_value(s):\n",
    "          match = re.search(r'ctr(\\d+)', s)\n",
    "          return int(match.group(1)) if match else -1  # fallback to -1 if no match\n",
    "\n",
    "     # Sort the keywords list based on the ctr number\n",
    "     keywords.sort(key=extract_ctr_value)\n",
    "\n",
    "     keywords = keywords[4:40]\n",
    "     \n",
    "     process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     #process_WAXS_data(WAXS_folder_name, keywords, data_folder, ai_w, mask=None, i0_offset=0, bstop_offset=0, i0_air=0, bstop_air=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     #Excluding the first 10 items from the keywords list (since the experiment starts around ctr 9)\n",
    "     #print(f\"keywords are {keywords}\")\n",
    "\n",
    "\n",
    "     #process_SAXS_data(SAXS_folder_name, keywords, data_folder, ai, saxs_mask, i0_offset, bstop_offset, i0_air=0, bstop_air=0)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWAXS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
